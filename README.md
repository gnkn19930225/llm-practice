# Seq2seq æ©Ÿå™¨ç¿»è­¯å°ˆæ¡ˆ (è‹±æ–‡è½‰è¥¿ç­ç‰™æ–‡)

æœ¬å°ˆæ¡ˆå¯¦ç¾äº†å…©ç¨®åºåˆ—åˆ°åºåˆ— (sequence-to-sequence) ç¿»è­¯æ¨¡å‹ï¼š
- **GRU æ¨¡å‹**: ä½¿ç”¨é–€æ§å¾ªç’°å–®å…ƒ (Gated Recurrent Unit)
- **Transformer æ¨¡å‹**: ä½¿ç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶ (Attention Mechanism)

## å°ˆæ¡ˆæ¶æ§‹åœ–

### GRU æ¨¡å‹æ¶æ§‹

```mermaid
graph TD
    A["è‹±æ–‡è¼¸å…¥<br/>Hello"] --> B["TextVectorization<br/>è©å½™è¡¨: 15000<br/>åºåˆ—é•·åº¦: 20"]
    C["è¥¿ç­ç‰™æ–‡è¼¸å‡º<br/>[start] Hola [end]"] --> D["TextVectorization<br/>è©å½™è¡¨: 15000<br/>åºåˆ—é•·åº¦: 21"]

    B --> E["æ•´æ•¸åºåˆ—<br/>[3, 15, 8, ...]"]
    D --> F["æ•´æ•¸åºåˆ—<br/>[2, 45, 12, ..., 3]"]

    E --> G["Embedding åµŒå…¥å±¤<br/>ç¶­åº¦: 256"]
    F --> H["Embedding åµŒå…¥å±¤<br/>ç¶­åº¦: 256<br/>mask_zero=True"]

    G --> I["Bidirectional GRU<br/>ç·¨ç¢¼å™¨<br/>éš±è—å±¤: 1024<br/>merge_mode=sum"]
    H --> J["GRU è§£ç¢¼å™¨<br/>éš±è—å±¤: 1024<br/>return_sequences=True"]

    I -->|"åˆå§‹ç‹€æ…‹<br/>initial_state"| J

    J --> K["Dropout<br/>rate: 0.5"]
    K --> L["Dense è¼¸å‡ºå±¤<br/>softmax<br/>15000 å€‹è©å½™"]
    L --> M["é æ¸¬çµæœ<br/>Hola"]

    style I fill:#e1f5ff
    style J fill:#fff4e1
    style L fill:#ffe1e1
```

### Transformer æ¨¡å‹æ¶æ§‹

```mermaid
graph TD
    A["è‹±æ–‡è¼¸å…¥<br/>Hello"] --> B["TextVectorization<br/>è©å½™è¡¨: 15000<br/>åºåˆ—é•·åº¦: 20"]
    C["è¥¿ç­ç‰™æ–‡è¼¸å‡º<br/>[start] Hola [end]"] --> D["TextVectorization<br/>è©å½™è¡¨: 15000<br/>åºåˆ—é•·åº¦: 21"]

    B --> E["æ•´æ•¸åºåˆ—<br/>[3, 15, 8, ...]"]
    D --> F["æ•´æ•¸åºåˆ—<br/>[2, 45, 12, ..., 3]"]

    E --> G["PositionalEmbedding<br/>è©åµŒå…¥ + ä½ç½®åµŒå…¥<br/>ç¶­åº¦: 256"]
    F --> H["PositionalEmbedding<br/>è©åµŒå…¥ + ä½ç½®åµŒå…¥<br/>ç¶­åº¦: 256"]

    G --> I["TransformerEncoder<br/>MultiHeadAttention (8é ­)<br/>Feed-Forward Network<br/>LayerNorm + æ®˜å·®é€£æ¥"]
    H --> J["TransformerDecoder<br/>Masked Self-Attention<br/>Cross-Attention<br/>Feed-Forward Network<br/>LayerNorm + æ®˜å·®é€£æ¥"]

    I -->|"ç·¨ç¢¼å™¨è¼¸å‡ºåºåˆ—<br/>(Cross-Attention)"| J

    J --> K["Dropout<br/>rate: 0.5"]
    K --> L["Dense è¼¸å‡ºå±¤<br/>softmax<br/>15000 å€‹è©å½™"]
    L --> M["é æ¸¬çµæœ<br/>Hola"]

    style G fill:#fff4e1
    style H fill:#fff4e1
    style I fill:#e1f5ff
    style J fill:#ffe1e1
    style L fill:#ffd4e1
```

**é—œéµå·®ç•°ï¼š**
- ğŸ”µ **GRU**: ä½¿ç”¨ Embeddingï¼ˆåªæœ‰è©åµŒå…¥ï¼‰
- ğŸŸ¡ **Transformer**: ä½¿ç”¨ PositionalEmbeddingï¼ˆè©åµŒå…¥ + ä½ç½®åµŒå…¥ï¼‰
- ğŸ”µ **GRU ç·¨ç¢¼å™¨**: è¼¸å‡ºå–®ä¸€ä¸Šä¸‹æ–‡å‘é‡ï¼Œé€šé initial_state å‚³é
- ğŸ”µ **Transformer ç·¨ç¢¼å™¨**: è¼¸å‡ºæ•´å€‹åºåˆ—ï¼Œé€šé Cross-Attention é€£æ¥
- ğŸ”´ **GRU è§£ç¢¼å™¨**: å–®ä¸€ GRU å±¤
- ğŸ”´ **Transformer è§£ç¢¼å™¨**: å…©å±¤æ³¨æ„åŠ›ï¼ˆMasked Self-Attention + Cross-Attentionï¼‰+ FFN
```

## æ¨¡å‹çµ„ä»¶èªªæ˜

### Embedding å±¤ (è©åµŒå…¥å±¤)

**ä½œç”¨**: å°‡æ•´æ•¸ç´¢å¼•è½‰æ›ç‚ºå¯†é›†çš„å‘é‡è¡¨ç¤º

```python
# ç·¨ç¢¼å™¨çš„åµŒå…¥å±¤
Embedding(vocab_size, embed_dim)  # (15000, 256)

# è§£ç¢¼å™¨çš„åµŒå…¥å±¤
Embedding(vocab_size, embed_dim, mask_zero=True)  # (15000, 256)
```

**ç‚ºä»€éº¼éœ€è¦ Embeddingï¼Ÿ**
- ç¥ç¶“ç¶²çµ¡ç„¡æ³•ç›´æ¥è™•ç†é›¢æ•£çš„æ•´æ•¸ç´¢å¼•
- å°‡ç¨€ç–çš„ one-hot ç·¨ç¢¼è½‰æ›ç‚ºå¯†é›†å‘é‡
- åœ¨è¨“ç·´ä¸­å­¸ç¿’è©å½™ä¹‹é–“çš„èªç¾©é—œä¿‚
- ç›¸ä¼¼çš„è©æœƒæœ‰ç›¸ä¼¼çš„å‘é‡è¡¨ç¤º

**è¼¸å…¥è¼¸å‡ºè½‰æ›ï¼š**
```
è¼¸å…¥ï¼šæ•´æ•¸åºåˆ— [3, 15, 8, 42]
     shape: (batch_size, sequence_length)

                    â†“  Embedding å±¤

è¼¸å‡ºï¼šåµŒå…¥å‘é‡
     shape: (batch_size, sequence_length, 256)
     æ¯å€‹æ•´æ•¸è®Šæˆ 256 ç¶­çš„æµ®é»æ•¸å‘é‡
```

**mask_zero=True çš„ä½œç”¨ï¼š**
- å°‡ç´¢å¼• `0` è¦–ç‚ºå¡«å……ç¬¦è™Ÿ (padding)
- åœ¨è¨ˆç®—æ™‚è‡ªå‹•å¿½ç•¥é€™äº›ä½ç½®
- ç”¨æ–¼è™•ç†ä¸åŒé•·åº¦çš„åºåˆ—

**ç¯„ä¾‹ï¼š**
```python
# è©å½™è¡¨: {"hello": 3, "world": 15, "good": 8, ...}
è¼¸å…¥ ID:  [3,    15,    8,    42   ]
         â†“     â†“      â†“     â†“
åµŒå…¥å‘é‡: [0.2,  [-0.1, [0.5,  [0.3,
          0.5,   0.3,   0.1,  -0.2,
          -0.1,  0.7,   0.4,   0.6,
          ...]   ...]   ...]   ...]
         256ç¶­  256ç¶­  256ç¶­  256ç¶­
```

---

### Bidirectional å±¤ (é›™å‘åŒ…è£å±¤)

**ä½œç”¨**: è®“ RNN åŒæ™‚å¾å…©å€‹æ–¹å‘è™•ç†åºåˆ—

```python
# ç·¨ç¢¼å™¨ä½¿ç”¨é›™å‘ GRU
encoded_source = Bidirectional(
    GRU(latent_dim),  # 1024 ç¶­
    merge_mode="sum"
)(x)
```

**ç‚ºä»€éº¼éœ€è¦ Bidirectionalï¼Ÿ**
- å–®å‘ RNN åªèƒ½çœ‹åˆ°"éå»"çš„ä¿¡æ¯
- é›™å‘å¯ä»¥åŒæ™‚çœ‹åˆ°å‰å¾Œæ–‡ï¼Œç²å¾—æ›´å®Œæ•´çš„ç†è§£
- ç‰¹åˆ¥é©åˆéœ€è¦ç†è§£æ•´å€‹å¥å­å«ç¾©çš„ä»»å‹™ï¼ˆå¦‚ç¿»è­¯ç·¨ç¢¼å™¨ï¼‰

**é‹ä½œæ–¹å¼ï¼š**
```
è¼¸å…¥åºåˆ—: ["I", "love", "you"]

å‰å‘ GRU (â†’):  I  â†’  love  â†’  you    (å¾å·¦åˆ°å³)
                                â†“
                            h_forward

å¾Œå‘ GRU (â†):  I  â†  love  â†  you    (å¾å³åˆ°å·¦)
                                â†“
                            h_backward

merge_mode="sum": h_final = h_forward + h_backward
```

**merge_mode åƒæ•¸æ¯”è¼ƒï¼š**

| merge_mode | èªªæ˜ | è¼¸å‡ºç¶­åº¦ | ç‰¹é» |
|------------|------|----------|------|
| `"sum"` | ç›¸åŠ  | 1024 | ç¯€çœåƒæ•¸ï¼Œçµåˆå…©æ–¹å‘ |
| `"concat"` | ä¸²æ¥ | 2048 | ä¿ç•™å®Œæ•´ä¿¡æ¯ |
| `"mul"` | ç›¸ä¹˜ | 1024 | å¼·èª¿å…±åŒç‰¹å¾µ |
| `"ave"` | å¹³å‡ | 1024 | å¹³è¡¡å…©æ–¹å‘ |
| `None` | åˆ†é–‹ | [1024, 1024] | åˆ†åˆ¥è™•ç† |

**ä½¿ç”¨æ™‚æ©Ÿï¼š**
- âœ… **é©åˆé›™å‘**: æ–‡æœ¬åˆ†é¡ã€æƒ…æ„Ÿåˆ†æã€**ç¿»è­¯ç·¨ç¢¼å™¨**
  - å¯ä»¥çœ‹åˆ°æ•´å€‹è¼¸å…¥åºåˆ—
  - éœ€è¦ç†è§£å®Œæ•´çš„ä¸Šä¸‹æ–‡

- âŒ **ä¸é©åˆé›™å‘**: æ–‡æœ¬ç”Ÿæˆã€**ç¿»è­¯è§£ç¢¼å™¨**
  - ç”Ÿæˆæ™‚ç„¡æ³•çœ‹åˆ°æœªä¾†çš„è©
  - å¿…é ˆæŒ‰é †åºé€æ­¥ç”Ÿæˆ

**åœ¨æœ¬å°ˆæ¡ˆä¸­ï¼š**
- **Encoder ç”¨é›™å‘**: å¯ä»¥å®Œæ•´ç†è§£è‹±æ–‡è¼¸å…¥å¥å­çš„å«ç¾©
- **Decoder ä¸ç”¨é›™å‘**: é€æ­¥ç”Ÿæˆè¥¿ç­ç‰™æ–‡ç¿»è­¯ï¼Œä¸èƒ½æå‰çœ‹åˆ°æœªä¾†çš„è©

---

### Encoder (ç·¨ç¢¼å™¨ - é›™å‘ GRU)
- **ä½œç”¨**: è™•ç†è‹±æ–‡è¼¸å…¥åºåˆ—
- **è¼¸å‡º**: å–®ä¸€ä¸Šä¸‹æ–‡å‘é‡ (encoded_source)
- **é›™å‘**: åŒæ™‚å¾å‰å‘å¾Œå’Œå¾å¾Œå‘å‰è®€å–åºåˆ—
- **merge_mode="sum"**: å°‡å…©å€‹æ–¹å‘çš„è¼¸å‡ºç›¸åŠ 

### Decoder (è§£ç¢¼å™¨ - GRU)
- **ä½œç”¨**: ç”Ÿæˆè¥¿ç­ç‰™æ–‡ç¿»è­¯
- **åˆå§‹ç‹€æ…‹**: ä½¿ç”¨ç·¨ç¢¼å™¨çš„ä¸Šä¸‹æ–‡å‘é‡
- **é€æ­¥é æ¸¬**: æ¯å€‹æ™‚é–“æ­¥é æ¸¬ä¸‹ä¸€å€‹è©
- **Teacher Forcing**: è¨“ç·´æ™‚ä½¿ç”¨æ­£ç¢ºç­”æ¡ˆä½œç‚ºè¼¸å…¥

---

## Seq2seq è¨“ç·´åŸç†ï¼šTeacher Forcing

### æ ¸å¿ƒæ¦‚å¿µ

**seq2seq æ¨¡å‹çš„è¼¸å…¥ï¼š**
1. **è‹±æ–‡åŸæ–‡** (ç·¨ç¢¼å™¨è¼¸å…¥)
2. **è¥¿ç­ç‰™æ–‡ç¿»è­¯çš„å‰åŠéƒ¨åˆ†** (è§£ç¢¼å™¨è¼¸å…¥)

**seq2seq æ¨¡å‹çš„è¼¸å‡ºï¼š**
- **è¥¿ç­ç‰™æ–‡ç¿»è­¯çš„ä¸‹ä¸€å€‹å­—** (é æ¸¬ç›®æ¨™)

### è¨“ç·´ç¯„ä¾‹

```python
# è¨“ç·´è³‡æ–™
è‹±æ–‡:         "Hello"
è¥¿ç­ç‰™æ–‡å®Œæ•´:  "[start] Hola [end]"

# æ¨¡å‹è¼¸å…¥è¼¸å‡ºï¼ˆéŒ¯ä½ä¸€å€‹å­—ï¼‰
è§£ç¢¼å™¨è¼¸å…¥:   "[start] Hola"        # spa[:, :-1] å»æ‰æœ€å¾Œä¸€å€‹å­—
é æ¸¬ç›®æ¨™:     "Hola [end]"          # spa[:, 1:]  å»æ‰ç¬¬ä¸€å€‹å­—
```

### é€æ­¥é æ¸¬éç¨‹

```
åŸå§‹åºåˆ—: [start]  Hola  [end]
           â†“       â†“      â†“
æ™‚é–“æ­¥ 0:  è¼¸å…¥ [start]           â†’ é æ¸¬ Hola
æ™‚é–“æ­¥ 1:  è¼¸å…¥ [start] Hola      â†’ é æ¸¬ [end]
```

**å®Œæ•´ç¯„ä¾‹ï¼š**
```
è‹±æ–‡: "Good morning"
è¥¿ç­ç‰™æ–‡: "[start] Buenos dÃ­as [end]"

è§£ç¢¼å™¨è¼¸å…¥åºåˆ—:  [start]  Buenos  dÃ­as
                  â†“       â†“       â†“
é æ¸¬ç›®æ¨™åºåˆ—:     Buenos  dÃ­as    [end]
```

### ç¨‹å¼ç¢¼å¯¦ç¾

```python
def format_dataset(eng, spa):
    eng = source_vectorization(eng)
    spa = target_vectorization(spa)
    return ({
            "english": eng,           # ç·¨ç¢¼å™¨è¼¸å…¥: è‹±æ–‡
            "spanish": spa[:, :-1]},  # è§£ç¢¼å™¨è¼¸å…¥: è¥¿ç­ç‰™æ–‡ï¼ˆå»æ‰æœ€å¾Œå­—ï¼‰
            spa[:, 1:])               # é æ¸¬ç›®æ¨™: è¥¿ç­ç‰™æ–‡ï¼ˆå»æ‰ç¬¬ä¸€å­—ï¼‰
```

### Teacher Forcing æ©Ÿåˆ¶

**è¨“ç·´æ™‚ (Teacher Forcing):**
```python
# å³ä½¿æ¨¡å‹é æ¸¬éŒ¯èª¤ï¼Œä»ä½¿ç”¨æ­£ç¢ºç­”æ¡ˆä½œç‚ºä¸‹ä¸€æ­¥è¼¸å…¥
æ™‚é–“æ­¥ 1: è¼¸å…¥ "[start]"      â†’ é æ¸¬ "Hola" âœ“
æ™‚é–“æ­¥ 2: è¼¸å…¥ "[start] Hola" â†’ é æ¸¬ "mundo" âœ— (éŒ¯èª¤)
æ™‚é–“æ­¥ 3: ä»ç„¶è¼¸å…¥æ­£ç¢ºçš„ "[start] Hola mundo" â†’ é æ¸¬ "[end]"
         (ä¸ä½¿ç”¨éŒ¯èª¤é æ¸¬çš„ "mundo")
```

**æ¨ç†æ™‚ (è‡ªå›æ­¸ç”Ÿæˆ):**
```python
# ä½¿ç”¨æ¨¡å‹è‡ªå·±çš„é æ¸¬ä½œç‚ºä¸‹ä¸€æ­¥è¼¸å…¥
æ™‚é–“æ­¥ 1: è¼¸å…¥ "[start]"      â†’ é æ¸¬ "Hola" âœ“
æ™‚é–“æ­¥ 2: è¼¸å…¥ "[start] Hola" â†’ é æ¸¬ "mundo" âœ— (éŒ¯èª¤)
æ™‚é–“æ­¥ 3: è¼¸å…¥éŒ¯èª¤çš„ "[start] Hola mundo" â†’ é æ¸¬å¯èƒ½ç¹¼çºŒéŒ¯èª¤
         (ä½¿ç”¨æ¨¡å‹è‡ªå·±çš„é æ¸¬ï¼ŒéŒ¯èª¤æœƒç´¯ç©)
```

### ç‚ºä»€éº¼éœ€è¦ Teacher Forcingï¼Ÿ

| ç‰¹é» | Teacher Forcing | è‡ªå›æ­¸ç”Ÿæˆ |
|------|-----------------|-----------|
| è¨“ç·´é€Ÿåº¦ | å¿«ï¼ˆä¸¦è¡Œè¨ˆç®—ï¼‰ | æ…¢ï¼ˆé€æ­¥ç”Ÿæˆï¼‰ |
| è¨“ç·´ç©©å®šæ€§ | é«˜ | ä½ï¼ˆéŒ¯èª¤ç´¯ç©ï¼‰ |
| ä½¿ç”¨æ™‚æ©Ÿ | è¨“ç·´éšæ®µ | æ¨ç†éšæ®µ |
| è¼¸å…¥ä¾†æº | æ­£ç¢ºç­”æ¡ˆ | æ¨¡å‹é æ¸¬ |

### æ¨ç†æ™‚çš„è§£ç¢¼éç¨‹

```python
def decode_sequence(input_sentence):
    decoded_sentence = "[start]"
    for i in range(max_decoded_sentence_length):
        # ä½¿ç”¨ç›®å‰å·²ç”Ÿæˆçš„å¥å­ä½œç‚ºè¼¸å…¥
        tokenized_target = target_vectorization([decoded_sentence])
        predictions = seq2seq.predict([input_sentence, tokenized_target])

        # é æ¸¬ä¸‹ä¸€å€‹å­—
        next_token = vocab[np.argmax(predictions[0, i, :])]
        decoded_sentence += " " + next_token

        if next_token == "[end]":
            break
    return decoded_sentence
```

**æ¨ç†ç¯„ä¾‹ï¼š**
```
è¼¸å…¥è‹±æ–‡: "Hello"

æ­¥é©Ÿ 1: decoded_sentence = "[start]"
       â†’ é æ¸¬ "Hola"
       â†’ decoded_sentence = "[start] Hola"

æ­¥é©Ÿ 2: decoded_sentence = "[start] Hola"
       â†’ é æ¸¬ "[end]"
       â†’ decoded_sentence = "[start] Hola [end]"

æœ€çµ‚è¼¸å‡º: "Hola"
```

### é—œéµåƒæ•¸
- **è©å½™è¡¨å¤§å°**: 15,000 tokens
- **åºåˆ—é•·åº¦**: 20 (è¼¸å…¥), 21 (ç›®æ¨™)
- **åµŒå…¥ç¶­åº¦**: 256
- **éš±è—å±¤ç¶­åº¦**: 1024
- **æ‰¹æ¬¡å¤§å°**: 64
- **è¨“ç·´è¼ªæ•¸**: 15 epochs

---

## GRU å…§éƒ¨çµæ§‹åœ–è§£

GRU (Gated Recurrent Unit) æ˜¯ä¸€ç¨®æ”¹è‰¯çš„ RNN æ¶æ§‹ï¼Œä½¿ç”¨é–€æ§æ©Ÿåˆ¶ä¾†æ§åˆ¶ä¿¡æ¯æµå‹•ã€‚

```mermaid
graph TB
    subgraph "GRU å–®å…ƒ (æ™‚é–“æ­¥ t)"
        Input["è¼¸å…¥ x(t)"]
        HiddenPrev["å‰ä¸€ç‹€æ…‹ h(t-1)"]

        Input --> Concat1["ä¸²æ¥"]
        HiddenPrev --> Concat1

        Concat1 --> ResetGate["é‡ç½®é–€ (Reset Gate)<br/>r = Ïƒ(WrÂ·[h(t-1), x(t)])"]
        Concat1 --> UpdateGate["æ›´æ–°é–€ (Update Gate)<br/>z = Ïƒ(WzÂ·[h(t-1), x(t)])"]

        ResetGate --> Multiply1["âŠ™<br/>é€å…ƒç´ ç›¸ä¹˜"]
        HiddenPrev --> Multiply1

        Multiply1 --> Concat2["ä¸²æ¥"]
        Input --> Concat2

        Concat2 --> CandidateState["å€™é¸ç‹€æ…‹<br/>hÌƒ(t) = tanh(WhÂ·[râŠ™h(t-1), x(t)])"]

        UpdateGate --> Multiply2["âŠ™<br/>1-z"]
        UpdateGate --> Multiply3["âŠ™<br/>z"]

        CandidateState --> Multiply2
        HiddenPrev --> Multiply3

        Multiply2 --> Add["âŠ•<br/>ç›¸åŠ "]
        Multiply3 --> Add

        Add --> Output["æ–°ç‹€æ…‹ h(t)<br/>h(t) = (1-z)âŠ™hÌƒ(t) + zâŠ™h(t-1)"]
    end

    style ResetGate fill:#ffe1e1
    style UpdateGate fill:#e1f5ff
    style CandidateState fill:#e1ffe1
    style Output fill:#fff4e1
```

### GRU ä¸‰å¤§çµ„ä»¶

#### 1. é‡ç½®é–€ (Reset Gate) - ç´…è‰²
```
r(t) = Ïƒ(Wr Â· [h(t-1), x(t)])
```
- **ä½œç”¨**: æ±ºå®šè¦å¿˜è¨˜å¤šå°‘éå»çš„ä¿¡æ¯
- **ç¯„åœ**: 0 åˆ° 1 (sigmoid æ¿€æ´»)
- **r â‰ˆ 0**: å¿½ç•¥éå»ç‹€æ…‹
- **r â‰ˆ 1**: ä¿ç•™éå»ç‹€æ…‹

#### 2. æ›´æ–°é–€ (Update Gate) - è—è‰²
```
z(t) = Ïƒ(Wz Â· [h(t-1), x(t)])
```
- **ä½œç”¨**: æ±ºå®šè¦ä¿ç•™å¤šå°‘èˆŠç‹€æ…‹ã€æ¥å—å¤šå°‘æ–°ç‹€æ…‹
- **ç¯„åœ**: 0 åˆ° 1 (sigmoid æ¿€æ´»)
- **z â‰ˆ 0**: æ›´æ–°ç‹€æ…‹ (æ¥å—æ–°ä¿¡æ¯)
- **z â‰ˆ 1**: ä¿æŒç‹€æ…‹ (å¿½ç•¥æ–°ä¿¡æ¯)

#### 3. å€™é¸ç‹€æ…‹ (Candidate State) - ç¶ è‰²
```
hÌƒ(t) = tanh(Wh Â· [r(t) âŠ™ h(t-1), x(t)])
```
- **ä½œç”¨**: è¨ˆç®—å€™é¸çš„æ–°ç‹€æ…‹
- **ç¯„åœ**: -1 åˆ° 1 (tanh æ¿€æ´»)
- **ä½¿ç”¨é‡ç½®é–€**: æ§åˆ¶éå»ä¿¡æ¯çš„å½±éŸ¿

### æœ€çµ‚è¼¸å‡º
```
h(t) = (1 - z(t)) âŠ™ hÌƒ(t) + z(t) âŠ™ h(t-1)
```
- **ç·šæ€§æ’å€¼**: åœ¨æ–°ç‹€æ…‹å’ŒèˆŠç‹€æ…‹ä¹‹é–“å–å¹³è¡¡
- **æ›´æ–°é–€æ§åˆ¶**: z æ±ºå®šæ–°èˆŠç‹€æ…‹çš„æ¯”ä¾‹

### GRU vs LSTM
| ç‰¹æ€§ | GRU | LSTM |
|------|-----|------|
| é–€çš„æ•¸é‡ | 2 å€‹ (é‡ç½®ã€æ›´æ–°) | 3 å€‹ (è¼¸å…¥ã€è¼¸å‡ºã€éºå¿˜) |
| åƒæ•¸é‡ | è¼ƒå°‘ | è¼ƒå¤š |
| è¨“ç·´é€Ÿåº¦ | è¼ƒå¿« | è¼ƒæ…¢ |
| è¨˜æ†¶èƒ½åŠ› | é©ä¸­ | è¼ƒå¼· |
| é©ç”¨å ´æ™¯ | ä¸­çŸ­åºåˆ— | é•·åºåˆ— |

## NLP Text Preprocessing Notes

### LLM æ–‡å­—å‰è™•ç†æ­¥é©Ÿ

1. **Tokenization (åˆ†è©)**
   - å°‡æ–‡æœ¬åˆ‡åˆ†æˆå–®è©æˆ–å­è©å–®å…ƒ
   - è™•ç†æ¨™é»ç¬¦è™Ÿå’Œç‰¹æ®Šå­—ç¬¦

2. **Normalization (æ­£è¦åŒ–)**
   - è½‰æ›ç‚ºå°å¯«
   - ç§»é™¤æˆ–çµ±ä¸€æ¨™é»ç¬¦è™Ÿ
   - è™•ç†æ•¸å­—å’Œç‰¹æ®Šç¬¦è™Ÿ

3. **Stop Words Removal (åœç”¨è©ç§»é™¤)**
   - ç§»é™¤å¸¸è¦‹ä½†ç„¡æ„ç¾©çš„è©å½™ï¼ˆå¦‚ï¼šthe, a, isï¼‰

4. **Stemming/Lemmatization (è©å¹¹æå–/è©å½¢é‚„åŸ)**
   - å°‡è©å½™é‚„åŸåˆ°åŸºæœ¬å½¢å¼

5. **å»ºç«‹è©å½™è¡¨ (Vocabulary Building)**
   - çµ±è¨ˆæ‰€æœ‰è¨“ç·´æ•¸æ“šä¸­çš„è©å½™
   - ä¾é »ç‡æ’åºï¼Œé¸å–æœ€å¸¸è¦‹çš„ N å€‹è©
   - ç‚ºæ¯å€‹è©åˆ†é…å”¯ä¸€çš„æ•´æ•¸ç´¢å¼• (token ID)
   - ä¿ç•™ç‰¹æ®Š tokenï¼ˆå¦‚ï¼š`[PAD]`, `[UNK]`, `[START]`, `[END]`ï¼‰

---

### target_vectorization.adapt() æ–¹æ³•

**ä½œç”¨**: åœ¨è¨“ç·´å‰åˆ†ææ–‡æœ¬æ•¸æ“šï¼Œå»ºç«‹è©å½™è¡¨ï¼ˆåŸ·è¡Œä¸Šè¿°æ­¥é©Ÿ 5ï¼‰

```python
target_vectorization = TextVectorization(
    max_tokens=vocab_size,
    output_mode="int",  # å¯é¸: "int", "multi_hot", "count", "tf_idf"
    output_sequence_length=sequence_length
)

# é‡è¦ï¼šä½¿ç”¨ adapt() å­¸ç¿’è©å½™è¡¨
target_vectorization.adapt(train_text_data)
```

**adapt() åšäº†ä»€éº¼:**
1. æƒææ‰€æœ‰è¨“ç·´æ•¸æ“š
2. çµ±è¨ˆè©å½™å‡ºç¾é »ç‡
3. å»ºç«‹è©å½™è¡¨ï¼ˆä¾é »ç‡æ’åºï¼Œä¿ç•™å‰ N å€‹æœ€å¸¸è¦‹çš„è©ï¼‰
4. ç‚ºæ¯å€‹è©åˆ†é…å”¯ä¸€çš„æ•´æ•¸ç´¢å¼•

**æ³¨æ„äº‹é …:**
- å¿…é ˆåœ¨è¨“ç·´å‰åŸ·è¡Œ
- åªèƒ½ç”¨è¨“ç·´æ•¸æ“šé€²è¡Œ adaptï¼ˆé¿å…æ•¸æ“šæ´©æ¼ï¼‰
- é©—è­‰é›†å’Œæ¸¬è©¦é›†ä½¿ç”¨ç›¸åŒçš„è©å½™è¡¨

---

### TextVectorization çš„ output_mode åƒæ•¸

è©å½™è¡¨å»ºç«‹å®Œæˆå¾Œï¼Œ**`output_mode` åƒæ•¸**æ±ºå®šäº†å¦‚ä½•å°‡æ–‡æœ¬è½‰æ›ç‚ºæ•¸å€¼ï¼š

#### 1. `output_mode="int"` (æ•´æ•¸åºåˆ—)
- **ç”¨é€”**: å°‡æ–‡æœ¬è½‰æ›ç‚ºæ•´æ•¸åºåˆ—ï¼Œä¿ç•™è©å½™é †åº
- **è¼¸å‡º**: `[3, 15, 8, 42, ...]` (æ¯å€‹æ•¸å­—ä»£è¡¨è©å½™è¡¨ä¸­çš„ç´¢å¼•)
- **é©åˆ**: åºåˆ—æ¨¡å‹ (RNN, LSTM, Transformer)
- **ç‰¹é»**: ä¿ç•™æ™‚é–“é †åºä¿¡æ¯

#### 2. `output_mode="multi_hot"` (Multi-Hot ç·¨ç¢¼)
- **ç”¨é€”**: åªè¨˜éŒ„å‡ºç¾éçš„å–®å­—ï¼ˆä¸çµ±è¨ˆæ¬¡æ•¸ï¼‰
- **è¼¸å‡º**: `[0, 1, 0, 1, 1, 0, ...]` (äºŒé€²åˆ¶å‘é‡ï¼Œ1 è¡¨ç¤ºè©²è©å‡ºç¾é)
- **é©åˆ**: ç°¡å–®åˆ†é¡ä»»å‹™
- **ç‰¹é»**: å¿½ç•¥è©é »å’Œé †åº

#### 3. `output_mode="count"` (è©é »çµ±è¨ˆ)
- **ç”¨é€”**: çµ±è¨ˆæ¯å€‹è©å‡ºç¾çš„æ¬¡æ•¸
- **è¼¸å‡º**: `[0, 3, 0, 1, 2, 0, ...]` (æ¯å€‹ä½ç½®çš„æ•¸å­—è¡¨ç¤ºè©²è©å‡ºç¾æ¬¡æ•¸)
- **é©åˆ**: å‚³çµ±æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ (å¦‚ Naive Bayes, SVM)
- **ç‰¹é»**: Bag of Words çš„æ¨™æº–å¯¦ä½œ

#### 4. `output_mode="tf_idf"` (TF-IDF æ¬Šé‡)
- **ç”¨é€”**: è¨ˆç®—è©å½™çš„ TF-IDF é‡è¦ç¨‹åº¦
- **è¼¸å‡º**: `[0, 0.23, 0, 0.89, 0.15, ...]` (æµ®é»æ•¸è¡¨ç¤ºè©çš„é‡è¦æ€§)
- **é©åˆ**: æ–‡æœ¬åˆ†é¡ã€ä¿¡æ¯æª¢ç´¢
- **ç‰¹é»**: éæ¿¾å¸¸è¦‹è©ï¼Œå¼·èª¿é‡è¦è©å½™

---

### Bag of Words (è©è¢‹æ¨¡å‹)

**åŸºæœ¬æ¦‚å¿µï¼š**
- è¨˜éŒ„æ¯å€‹è©åœ¨æ–‡æª”ä¸­å‡ºç¾çš„æ¬¡æ•¸
- å¿½ç•¥è©å½™çš„é †åºï¼Œåªé—œæ³¨å‡ºç¾é »ç‡
- å°‡æ–‡æœ¬è½‰æ›ç‚ºæ•¸å€¼å‘é‡
- å°æ‡‰ `output_mode="count"` æˆ– `"multi_hot"`

**è©å½™é‡è¦ç¨‹åº¦è¨ˆç®— (TF-IDF):**
- **TF (Term Frequency)**: è©åœ¨å–®ç¯‡æ–‡æª”ä¸­çš„å‡ºç¾æ¬¡æ•¸
- **IDF (Inverse Document Frequency)**: `log(ç¸½æ–‡æª”æ•¸ / åŒ…å«è©²è©çš„æ–‡æª”æ•¸)`
- **TF-IDF**: `TF Ã— IDF`
  - è¡¡é‡è©å½™åœ¨æ–‡æª”ä¸­çš„é‡è¦æ€§
  - å…¬å¼: è©åœ¨æ–‡ç« ä¸­çš„é‡è¦æ€§ = (è©åœ¨è©²æ–‡ç« çš„å‡ºç¾æ¬¡æ•¸) Ã— log(ç¸½æ–‡æª”æ•¸ / åŒ…å«è©²è©çš„æ–‡æª”æ•¸)
  - éæ¿¾æ‰éæ–¼å¸¸è¦‹æˆ–éæ–¼ç½•è¦‹çš„è©
- å°æ‡‰ `output_mode="tf_idf"`

---

### æ¨¡å‹é¸æ“‡æº–å‰‡ï¼šåºåˆ—æ¨¡å‹ vs è©è¢‹æ¨¡å‹

æ ¹æ“šæ•¸æ“šç‰¹æ€§é¸æ“‡åˆé©çš„æ¨¡å‹ï¼š

**ä½¿ç”¨åºåˆ—æ¨¡å‹ (RNN/LSTM/Transformer) ç•¶:**
- **æ¨£æœ¬æ•¸é‡ / æ¨£æœ¬å¹³å‡é•·åº¦ > 1500**
- è©å½™é †åºå¾ˆé‡è¦ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æã€ç¿»è­¯ï¼‰
- éœ€è¦æ•æ‰ä¸Šä¸‹æ–‡ä¾è³´é—œä¿‚

**ä½¿ç”¨è©è¢‹æ¨¡å‹ (Bag of Words) ç•¶:**
- **æ¨£æœ¬æ•¸é‡ / æ¨£æœ¬å¹³å‡é•·åº¦ < 1500**
- æ•¸æ“šé‡è¼ƒå°
- è©å½™å‡ºç¾èˆ‡å¦æ¯”é †åºæ›´é‡è¦ï¼ˆå¦‚åƒåœ¾éƒµä»¶åˆ†é¡ï¼‰
- éœ€è¦å¿«é€Ÿè¨“ç·´å’Œæ¨ç†

**åˆ¤æ–·å…¬å¼:**
```
å¦‚æœ (ç¸½æ¨£æœ¬æ•¸ / å¹³å‡æ–‡æœ¬é•·åº¦) > 1500:
    ä½¿ç”¨åºåˆ—æ¨¡å‹ (output_mode="int")
å¦å‰‡:
    ä½¿ç”¨è©è¢‹æ¨¡å‹ (output_mode="count" æˆ– "tf_idf")
```

---

---

## Transformer ç¿»è­¯æ¨¡å‹

### æ ¸å¿ƒæ¦‚å¿µ

Transformer ä½¿ç”¨**æ³¨æ„åŠ›æ©Ÿåˆ¶ (Attention Mechanism)** å–ä»£ RNN çš„é †åºè™•ç†ï¼Œå¯¦ç¾æ›´é«˜æ•ˆçš„åºåˆ—åˆ°åºåˆ—ç¿»è­¯ã€‚

### æ¶æ§‹å·®ç•°ï¼šGRU vs Transformer

| ç‰¹æ€§ | GRU æ¨¡å‹ | Transformer æ¨¡å‹ |
|------|----------|------------------|
| **æ ¸å¿ƒæ©Ÿåˆ¶** | å¾ªç’°ç¥ç¶“ç¶²çµ¡ (RNN) | è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ (Self-Attention) |
| **è™•ç†æ–¹å¼** | é †åºè™•ç† (Sequential) | ä¸¦è¡Œè™•ç† (Parallel) |
| **ä½ç½®ä¿¡æ¯** | å¤©ç”Ÿå…·å‚™é †åºä¿¡æ¯ | éœ€è¦ PositionalEmbedding |
| **é•·è·é›¢ä¾è³´** | é€šééš±è—ç‹€æ…‹å‚³é | ç›´æ¥é€£æ¥æ‰€æœ‰ä½ç½® |

### PositionalEmbedding è§£æ

**ç‚ºä»€éº¼ Transformer éœ€è¦ PositionalEmbeddingï¼Ÿ**

```python
class PositionalEmbedding(tf.keras.layers.Layer):
    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):
        super().__init__(**kwargs)
        # Token Embedding: è©çš„èªç¾©ä¿¡æ¯
        self.token_embeddings = tf.keras.layers.Embedding(
            input_dim=input_dim, output_dim=output_dim)
        # Position Embedding: è©çš„ä½ç½®ä¿¡æ¯
        self.position_embeddings = tf.keras.layers.Embedding(
            input_dim=sequence_length, output_dim=output_dim)

    def call(self, inputs):
        # å°‡è©åµŒå…¥å’Œä½ç½®åµŒå…¥ç›¸åŠ 
        embedded_tokens = self.token_embeddings(inputs)
        embedded_positions = self.position_embeddings(positions)
        return embedded_tokens + embedded_positions
```

**GRU vs Transformer è™•ç†é †åºçš„æ–¹å¼ï¼š**

```
å¥å­: "I love you"

GRU è™•ç†:
â”œâ”€ æ™‚é–“æ­¥ 1: è™•ç† "I"    â†’ æ›´æ–°éš±è—ç‹€æ…‹
â”œâ”€ æ™‚é–“æ­¥ 2: è™•ç† "love" â†’ æ›´æ–°éš±è—ç‹€æ…‹ï¼ˆçŸ¥é“åœ¨ "I" å¾Œé¢ï¼‰
â””â”€ æ™‚é–“æ­¥ 3: è™•ç† "you"  â†’ æ›´æ–°éš±è—ç‹€æ…‹ï¼ˆçŸ¥é“åœ¨ "love" å¾Œé¢ï¼‰
âœ“ å¤©ç”Ÿå°±çŸ¥é“è©çš„é †åº

Transformer è™•ç† (æ²’æœ‰ PositionalEmbedding):
â”œâ”€ åŒæ™‚çœ‹åˆ°: ["I", "love", "you"]
â””â”€ âœ— ç„¡æ³•å€åˆ† "I love you" å’Œ "you love I"

Transformer è™•ç† (æœ‰ PositionalEmbedding):
â”œâ”€ åŒæ™‚çœ‹åˆ°: ["I"+ä½ç½®0, "love"+ä½ç½®1, "you"+ä½ç½®2]
â””â”€ âœ“ ç¾åœ¨çŸ¥é“æ¯å€‹è©çš„ä½ç½®äº†ï¼
```

**çµè«–**: GRU é †åºè™•ç†å¤©ç”ŸçŸ¥é“è©åºï¼ŒTransformer ä¸¦è¡Œè™•ç†éœ€è¦æ˜ç¢ºæ·»åŠ ä½ç½®ä¿¡æ¯ã€‚

---

### Transformer æ ¸å¿ƒçµ„ä»¶

#### 1. TransformerEncoder (ç·¨ç¢¼å™¨)
- **MultiHeadAttention**: å¤šå€‹æ³¨æ„åŠ›é ­åŒæ™‚é—œæ³¨ä¸åŒç‰¹å¾µ
- **Feed-Forward Network**: å…¨é€£æ¥å‰é¥‹ç¶²çµ¡
- **LayerNormalization**: ç©©å®šè¨“ç·´éç¨‹
- **æ®˜å·®é€£æ¥**: ç·©è§£æ¢¯åº¦æ¶ˆå¤±å•é¡Œ

**ä½œç”¨**:
- ä½¿ç”¨è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ç†è§£è¼¸å…¥å¥å­
- æ¯å€‹è©å¯ä»¥ç›´æ¥é—œæ³¨å¥å­ä¸­çš„æ‰€æœ‰å…¶ä»–è©
- ä¸¦è¡Œè™•ç†ï¼Œæ¯” GRU æ›´å¿«

#### 2. TransformerDecoder (è§£ç¢¼å™¨)
- **Masked Self-Attention**: ä½¿ç”¨ causal mask é˜²æ­¢çœ‹åˆ°æœªä¾†ä¿¡æ¯
- **Cross-Attention**: é—œæ³¨ç·¨ç¢¼å™¨çš„è¼¸å‡º
- **Feed-Forward Network**: å…¨é€£æ¥å‰é¥‹ç¶²çµ¡
- **LayerNormalization + æ®˜å·®é€£æ¥**

**Causal Maskingï¼ˆå› æœé®ç½©ï¼‰**:
```
ç”Ÿæˆ "Hola mundo" æ™‚çš„æ³¨æ„åŠ›é®ç½©ï¼š

         Hola  mundo  [end]
Hola      âœ“     âœ—      âœ—     (åªèƒ½çœ‹åˆ° "Hola")
mundo     âœ“     âœ“      âœ—     (å¯ä»¥çœ‹åˆ° "Hola" å’Œ "mundo")
[end]     âœ“     âœ“      âœ“     (å¯ä»¥çœ‹åˆ°æ‰€æœ‰å·²ç”Ÿæˆçš„è©)

âœ“ å¯ä»¥é—œæ³¨    âœ— è¢«é®ç½©ï¼ˆé˜²æ­¢çœ‹åˆ°æœªä¾†ï¼‰
```

**ä½œç”¨**:
- ç¢ºä¿ç”Ÿæˆæ¯å€‹è©æ™‚ï¼Œåªèƒ½çœ‹åˆ°ä¹‹å‰å·²ç”Ÿæˆçš„è©
- è¨“ç·´æ™‚æ¨¡æ“¬çœŸå¯¦æ¨ç†å ´æ™¯
- é¿å…ä¿¡æ¯æ´©æ¼

---

### è§£ç¢¼ç­–ç•¥

å…©ç¨®æ¨¡å‹éƒ½ä½¿ç”¨ **Greedy Decodingï¼ˆè²ªå©ªè§£ç¢¼ï¼‰**ï¼š
```python
def decode_sequence(input_sentence):
    decoded_sentence = "[start]"
    for i in range(max_decoded_sentence_length):
        predictions = model.predict([input_sentence, decoded_sentence])
        # æ¯æ¬¡é¸æ“‡æ¦‚ç‡æœ€é«˜çš„è©
        next_token = vocab[np.argmax(predictions[0, i, :])]
        decoded_sentence += " " + next_token
        if next_token == "[end]":
            break
    return decoded_sentence
```

---

### ä½•æ™‚é¸æ“‡å“ªç¨®æ¨¡å‹ï¼Ÿ

**é¸æ“‡ GRU ç•¶:**
- âœ“ æ•¸æ“šé‡è¼ƒå°
- âœ“ éœ€è¦å¿«é€Ÿè¨“ç·´
- âœ“ çŸ­åˆ°ä¸­ç­‰é•·åº¦çš„å¥å­
- âœ“ è³‡æºå—é™çš„ç’°å¢ƒ

**é¸æ“‡ Transformer ç•¶:**
- âœ“ æ•¸æ“šé‡å……è¶³
- âœ“ é•·å¥å­æˆ–è¤‡é›œèªæ³•
- âœ“ è¿½æ±‚æœ€ä½³ç¿»è­¯å“è³ª
- âœ“ æœ‰å……è¶³çš„è¨ˆç®—è³‡æº

---

## å°ˆæ¡ˆæª”æ¡ˆ

- `seq2seq_gru_translation.py`: GRU ç¿»è­¯æ¨¡å‹è¨“ç·´è…³æœ¬
- `transformer_translation.py`: Transformer ç¿»è­¯æ¨¡å‹è¨“ç·´è…³æœ¬
- `spa-eng/`: è‹±æ–‡-è¥¿ç­ç‰™æ–‡å¹³è¡Œèªæ–™åº«è³‡æ–™å¤¾
- `spa-eng/spa.txt`: è¨“ç·´è³‡æ–™ (æ ¼å¼: English\tSpanish)

## ä½¿ç”¨æ–¹å¼

```bash
# GRU ç‰ˆæœ¬
python seq2seq_gru_translation.py

# Transformer ç‰ˆæœ¬
python transformer_translation.py
```

## ç›¸ä¾å¥—ä»¶

- tensorflow/keras
- Python 3.x

## è³‡æ–™é›†æ ¼å¼

```
Hello.\tÂ¡Hola!
How are you?\tÂ¿CÃ³mo estÃ¡s?
Good morning.\tBuenos dÃ­as.
```

æ¯è¡ŒåŒ…å«è‹±æ–‡å’Œè¥¿ç­ç‰™æ–‡ï¼Œä»¥ tab (`\t`) åˆ†éš”ã€‚
